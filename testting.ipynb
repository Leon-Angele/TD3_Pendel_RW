{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b173a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Num: 1 Episode T: 500 Reward: -3025.774\n",
      "Episode Num: 2 Episode T: 500 Reward: -2240.064\n",
      "Episode Num: 2 Episode T: 500 Reward: -2240.064\n",
      "Episode Num: 3 Episode T: 500 Reward: -2720.860\n",
      "Episode Num: 3 Episode T: 500 Reward: -2720.860\n",
      "Episode Num: 4 Episode T: 500 Reward: -2926.188\n",
      "Episode Num: 4 Episode T: 500 Reward: -2926.188\n",
      "Episode Num: 5 Episode T: 500 Reward: -2559.347\n",
      "Episode Num: 5 Episode T: 500 Reward: -2559.347\n",
      "Episode Num: 6 Episode T: 500 Reward: -2833.149\n",
      "Episode Num: 6 Episode T: 500 Reward: -2833.149\n",
      "Episode Num: 7 Episode T: 500 Reward: -2532.866\n",
      "Episode Num: 7 Episode T: 500 Reward: -2532.866\n",
      "Episode Num: 8 Episode T: 500 Reward: -2529.303\n",
      "Episode Num: 8 Episode T: 500 Reward: -2529.303\n",
      "Episode Num: 9 Episode T: 500 Reward: -2295.750\n",
      "Episode Num: 9 Episode T: 500 Reward: -2295.750\n",
      "Episode Num: 10 Episode T: 500 Reward: -2133.608\n",
      "Episode Num: 10 Episode T: 500 Reward: -2133.608\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -3941.752\n",
      "---------------------------------------\n",
      "Episode Num: 11 Episode T: 500 Reward: -3105.221\n",
      "Evaluation over 10 episodes: -3941.752\n",
      "---------------------------------------\n",
      "Episode Num: 11 Episode T: 500 Reward: -3105.221\n",
      "Episode Num: 12 Episode T: 500 Reward: -2900.876\n",
      "Episode Num: 12 Episode T: 500 Reward: -2900.876\n",
      "Episode Num: 13 Episode T: 500 Reward: -2836.770\n",
      "Episode Num: 13 Episode T: 500 Reward: -2836.770\n",
      "Episode Num: 14 Episode T: 500 Reward: -2954.092\n",
      "Episode Num: 14 Episode T: 500 Reward: -2954.092\n",
      "Episode Num: 15 Episode T: 500 Reward: -3752.890\n",
      "Episode Num: 15 Episode T: 500 Reward: -3752.890\n",
      "Episode Num: 16 Episode T: 500 Reward: -3290.642\n",
      "Episode Num: 16 Episode T: 500 Reward: -3290.642\n",
      "Episode Num: 17 Episode T: 500 Reward: -2424.268\n",
      "Episode Num: 17 Episode T: 500 Reward: -2424.268\n",
      "Episode Num: 18 Episode T: 500 Reward: -2747.792\n",
      "Episode Num: 18 Episode T: 500 Reward: -2747.792\n",
      "Episode Num: 19 Episode T: 500 Reward: -2354.877\n",
      "Episode Num: 19 Episode T: 500 Reward: -2354.877\n",
      "Episode Num: 20 Episode T: 500 Reward: -2165.883\n",
      "Episode Num: 20 Episode T: 500 Reward: -2165.883\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -244.027\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -244.027\n",
      "---------------------------------------\n",
      "Episode Num: 21 Episode T: 500 Reward: -8.908\n",
      "Episode Num: 21 Episode T: 500 Reward: -8.908\n",
      "Episode Num: 22 Episode T: 500 Reward: -55.890\n",
      "Episode Num: 22 Episode T: 500 Reward: -55.890\n",
      "Episode Num: 23 Episode T: 500 Reward: -123.115\n",
      "Episode Num: 23 Episode T: 500 Reward: -123.115\n",
      "Episode Num: 24 Episode T: 500 Reward: -1.406\n",
      "Episode Num: 24 Episode T: 500 Reward: -1.406\n",
      "Episode Num: 25 Episode T: 500 Reward: -150.416\n",
      "Episode Num: 25 Episode T: 500 Reward: -150.416\n",
      "Episode Num: 26 Episode T: 500 Reward: -163.597\n",
      "Episode Num: 26 Episode T: 500 Reward: -163.597\n",
      "Episode Num: 27 Episode T: 500 Reward: -113.976\n",
      "Episode Num: 27 Episode T: 500 Reward: -113.976\n",
      "Episode Num: 28 Episode T: 500 Reward: -122.122\n",
      "Episode Num: 28 Episode T: 500 Reward: -122.122\n",
      "Episode Num: 29 Episode T: 500 Reward: -291.572\n",
      "Episode Num: 29 Episode T: 500 Reward: -291.572\n",
      "Episode Num: 30 Episode T: 500 Reward: -126.604\n",
      "Episode Num: 30 Episode T: 500 Reward: -126.604\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -143.805\n",
      "---------------------------------------\n",
      "Episode Num: 31 Episode T: 500 Reward: -1.706\n",
      "Evaluation over 10 episodes: -143.805\n",
      "---------------------------------------\n",
      "Episode Num: 31 Episode T: 500 Reward: -1.706\n",
      "Episode Num: 32 Episode T: 500 Reward: -93.150\n",
      "Episode Num: 32 Episode T: 500 Reward: -93.150\n",
      "Episode Num: 33 Episode T: 500 Reward: -10.450\n",
      "Episode Num: 33 Episode T: 500 Reward: -10.450\n",
      "Episode Num: 34 Episode T: 500 Reward: -110.636\n",
      "Episode Num: 34 Episode T: 500 Reward: -110.636\n",
      "Episode Num: 35 Episode T: 500 Reward: -147.850\n",
      "Episode Num: 35 Episode T: 500 Reward: -147.850\n",
      "Episode Num: 36 Episode T: 500 Reward: -241.297\n",
      "Episode Num: 36 Episode T: 500 Reward: -241.297\n",
      "Episode Num: 37 Episode T: 500 Reward: -130.367\n",
      "Episode Num: 37 Episode T: 500 Reward: -130.367\n",
      "Episode Num: 38 Episode T: 500 Reward: -124.827\n",
      "Episode Num: 38 Episode T: 500 Reward: -124.827\n",
      "Episode Num: 39 Episode T: 500 Reward: -142.265\n",
      "Episode Num: 39 Episode T: 500 Reward: -142.265\n",
      "Episode Num: 40 Episode T: 500 Reward: -265.268\n",
      "Episode Num: 40 Episode T: 500 Reward: -265.268\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -107.995\n",
      "---------------------------------------\n",
      "Episode Num: 41 Episode T: 500 Reward: -0.443\n",
      "Evaluation over 10 episodes: -107.995\n",
      "---------------------------------------\n",
      "Episode Num: 41 Episode T: 500 Reward: -0.443\n",
      "Episode Num: 42 Episode T: 500 Reward: -148.291\n",
      "Episode Num: 42 Episode T: 500 Reward: -148.291\n",
      "Episode Num: 43 Episode T: 500 Reward: -62.893\n",
      "Episode Num: 43 Episode T: 500 Reward: -62.893\n",
      "Episode Num: 44 Episode T: 500 Reward: -29.499\n",
      "Episode Num: 44 Episode T: 500 Reward: -29.499\n",
      "Episode Num: 45 Episode T: 500 Reward: -146.230\n",
      "Episode Num: 45 Episode T: 500 Reward: -146.230\n",
      "Episode Num: 46 Episode T: 500 Reward: -79.773\n",
      "Episode Num: 46 Episode T: 500 Reward: -79.773\n",
      "Episode Num: 47 Episode T: 500 Reward: -290.960\n",
      "Episode Num: 47 Episode T: 500 Reward: -290.960\n",
      "Episode Num: 48 Episode T: 500 Reward: -131.444\n",
      "Episode Num: 48 Episode T: 500 Reward: -131.444\n",
      "Episode Num: 49 Episode T: 500 Reward: -76.114\n",
      "Episode Num: 49 Episode T: 500 Reward: -76.114\n",
      "Episode Num: 50 Episode T: 500 Reward: -93.532\n",
      "Episode Num: 50 Episode T: 500 Reward: -93.532\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -119.792\n",
      "---------------------------------------\n",
      "Episode Num: 51 Episode T: 500 Reward: -2.365\n",
      "Evaluation over 10 episodes: -119.792\n",
      "---------------------------------------\n",
      "Episode Num: 51 Episode T: 500 Reward: -2.365\n",
      "Episode Num: 52 Episode T: 500 Reward: -17.371\n",
      "Episode Num: 52 Episode T: 500 Reward: -17.371\n",
      "Episode Num: 53 Episode T: 500 Reward: -105.996\n",
      "Episode Num: 53 Episode T: 500 Reward: -105.996\n",
      "Episode Num: 54 Episode T: 500 Reward: -23.878\n",
      "Episode Num: 54 Episode T: 500 Reward: -23.878\n",
      "Episode Num: 55 Episode T: 500 Reward: -215.893\n",
      "Episode Num: 55 Episode T: 500 Reward: -215.893\n",
      "Episode Num: 56 Episode T: 500 Reward: -4.532\n",
      "Episode Num: 56 Episode T: 500 Reward: -4.532\n",
      "Episode Num: 57 Episode T: 500 Reward: -258.473\n",
      "Episode Num: 57 Episode T: 500 Reward: -258.473\n",
      "Episode Num: 58 Episode T: 500 Reward: -164.822\n",
      "Episode Num: 58 Episode T: 500 Reward: -164.822\n",
      "Episode Num: 59 Episode T: 500 Reward: -130.772\n",
      "Episode Num: 59 Episode T: 500 Reward: -130.772\n",
      "Episode Num: 60 Episode T: 500 Reward: -119.177\n",
      "Episode Num: 60 Episode T: 500 Reward: -119.177\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -103.386\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -103.386\n",
      "---------------------------------------\n",
      "Episode Num: 61 Episode T: 500 Reward: -2.473\n",
      "Episode Num: 61 Episode T: 500 Reward: -2.473\n",
      "Episode Num: 62 Episode T: 500 Reward: -234.512\n",
      "Episode Num: 62 Episode T: 500 Reward: -234.512\n",
      "Episode Num: 63 Episode T: 500 Reward: -18.145\n",
      "Episode Num: 63 Episode T: 500 Reward: -18.145\n",
      "Episode Num: 64 Episode T: 500 Reward: -174.835\n",
      "Episode Num: 64 Episode T: 500 Reward: -174.835\n",
      "Episode Num: 65 Episode T: 500 Reward: -227.478\n",
      "Episode Num: 65 Episode T: 500 Reward: -227.478\n",
      "Episode Num: 66 Episode T: 500 Reward: -105.023\n",
      "Episode Num: 66 Episode T: 500 Reward: -105.023\n",
      "Episode Num: 67 Episode T: 500 Reward: -131.991\n",
      "Episode Num: 67 Episode T: 500 Reward: -131.991\n",
      "Episode Num: 68 Episode T: 500 Reward: -7.916\n",
      "Episode Num: 68 Episode T: 500 Reward: -7.916\n",
      "Episode Num: 69 Episode T: 500 Reward: -106.530\n",
      "Episode Num: 69 Episode T: 500 Reward: -106.530\n",
      "Episode Num: 70 Episode T: 500 Reward: -274.029\n",
      "Episode Num: 70 Episode T: 500 Reward: -274.029\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.154\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.154\n",
      "---------------------------------------\n",
      "Episode Num: 71 Episode T: 500 Reward: -2.123\n",
      "Episode Num: 71 Episode T: 500 Reward: -2.123\n",
      "Episode Num: 72 Episode T: 500 Reward: -126.590\n",
      "Episode Num: 72 Episode T: 500 Reward: -126.590\n",
      "Episode Num: 73 Episode T: 500 Reward: -27.385\n",
      "Episode Num: 73 Episode T: 500 Reward: -27.385\n",
      "Episode Num: 74 Episode T: 500 Reward: -91.930\n",
      "Episode Num: 74 Episode T: 500 Reward: -91.930\n",
      "Episode Num: 75 Episode T: 500 Reward: -174.000\n",
      "Episode Num: 75 Episode T: 500 Reward: -174.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "# Constants for the Pendulum environment\n",
    "G = 10.0  # Gravity\n",
    "L = 1.0   # Length of pendulum\n",
    "M = 1.0   # Mass\n",
    "DT = 0.05  # Time step\n",
    "MAX_TORQUE = 2.0\n",
    "MAX_SPEED = 8.0\n",
    "\n",
    "class PendulumEnv:\n",
    "    def __init__(self, render=False):\n",
    "        self.state = None\n",
    "        self.render_flag = render\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        if self.render_flag:\n",
    "            import pygame\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((500, 500))\n",
    "            pygame.display.set_caption(\"Pendulum Simulation\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        high = np.array([np.pi, 1])\n",
    "        self.state = np.random.uniform(-high, high)\n",
    "        self.state[1] *= MAX_SPEED  # theta_dot\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        theta, theta_dot = self.state\n",
    "        return np.array([np.cos(theta), np.sin(theta), theta_dot], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, -MAX_TORQUE, MAX_TORQUE)\n",
    "        if isinstance(action, np.ndarray) and action.ndim > 0:\n",
    "            action = action[0]\n",
    "        theta, theta_dot = self.state\n",
    "        g = G\n",
    "        l = L\n",
    "        m = M\n",
    "        dt = DT\n",
    "\n",
    "        new_theta_dot = theta_dot + (3 * g / (2 * l) * np.sin(theta) + 3. / (m * l ** 2) * action) * dt\n",
    "        new_theta_dot = np.clip(new_theta_dot, -MAX_SPEED, MAX_SPEED)\n",
    "        new_theta = theta + new_theta_dot * dt\n",
    "\n",
    "        self.state = np.array([new_theta, new_theta_dot])\n",
    "\n",
    "        # Normalize theta to [-pi, pi]\n",
    "        self.state[0] = ((self.state[0] + np.pi) % (2 * np.pi)) - np.pi\n",
    "\n",
    "        # Reward calculation (same as Gym Pendulum-v1)\n",
    "        cost = self._angle_normalize(self.state[0]) ** 2 + 0.1 * self.state[1] ** 2 + 0.001 * (action ** 2)\n",
    "        reward = -cost\n",
    "\n",
    "        # Done: In custom env, we set no termination by default, but will handle in loop\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        return self._get_obs(), reward, terminated, truncated, {}\n",
    "\n",
    "    def _angle_normalize(self, x):\n",
    "        return ((x + np.pi) % (2 * np.pi)) - np.pi\n",
    "\n",
    "    def render(self):\n",
    "        if not self.render_flag:\n",
    "            return\n",
    "        import pygame\n",
    "        self.screen.fill((255, 255, 255))\n",
    "        theta = self.state[0]\n",
    "        # Draw pendulum\n",
    "        origin = (250, 250)\n",
    "        end = (250 + 200 * np.sin(theta), 250 + 200 * np.cos(theta))\n",
    "        pygame.draw.line(self.screen, (0, 0, 0), origin, end, 8)\n",
    "        pygame.draw.circle(self.screen, (0, 0, 255), end, 20)\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(60)  # 60 FPS\n",
    "\n",
    "    def close(self):\n",
    "        if self.render_flag:\n",
    "            import pygame\n",
    "            pygame.quit()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l5 = nn.Linear(256, 256)\n",
    "        self.l6 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        q2 = F.relu(self.l4(sa))\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        return q1, q2\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        return q1\n",
    "\n",
    "class TD3(object):\n",
    "    def __init__(self, state_dim, action_dim, max_action, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_freq = policy_freq\n",
    "\n",
    "        self.total_it = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=256):\n",
    "        self.total_it += 1\n",
    "\n",
    "        # Sample replay buffer \n",
    "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Select action according to policy and add clipped noise\n",
    "            noise = (\n",
    "                torch.randn_like(action) * self.policy_noise\n",
    "            ).clamp(-self.noise_clip, self.noise_clip)\n",
    "            \n",
    "            next_action = (\n",
    "                self.actor_target(next_state) + noise\n",
    "            ).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "        # Get current Q estimates\n",
    "        current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Delayed policy updates\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "\n",
    "            # Compute actor loss\n",
    "            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "            \n",
    "            # Optimize the actor \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update the frozen target models\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "        self.state = np.zeros((max_size, state_dim))\n",
    "        self.action = np.zeros((max_size, action_dim))\n",
    "        self.next_state = np.zeros((max_size, state_dim))\n",
    "        self.reward = np.zeros((max_size, 1))\n",
    "        self.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        self.state[self.ptr] = state\n",
    "        self.action[self.ptr] = action\n",
    "        self.next_state[self.ptr] = next_state\n",
    "        self.reward[self.ptr] = reward\n",
    "        self.not_done[self.ptr] = 1. - done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "        return (\n",
    "            torch.FloatTensor(self.state[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.action[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "        )\n",
    "\n",
    "# Runs policy for X episodes and returns average reward\n",
    "def eval_policy(policy, env, eval_episodes=10, max_steps=500):\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0.\n",
    "        for _ in range(max_steps):\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        avg_reward += episode_reward\n",
    "    avg_reward /= eval_episodes\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    return avg_reward\n",
    "\n",
    "# Main script\n",
    "env = PendulumEnv(render=False)  # No rendering during training\n",
    "\n",
    "state_dim = 3  # [cos(theta), sin(theta), theta_dot]\n",
    "action_dim = 1 \n",
    "max_action = MAX_TORQUE\n",
    "\n",
    "policy = TD3(state_dim, action_dim, max_action)\n",
    "\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "\n",
    "# Parameters\n",
    "start_timesteps = 10000\n",
    "max_episodes = 75  # Total episodes\n",
    "max_steps = 500  # Timesteps per episode\n",
    "eval_freq = 10  # Evaluate every 10 episodes\n",
    "expl_noise = 0.15\n",
    "batch_size = 256\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "state, _ = env.reset()\n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0\n",
    "total_timesteps = 0\n",
    "\n",
    "while episode_num < max_episodes:\n",
    "    episode_timesteps += 1\n",
    "    total_timesteps += 1\n",
    "\n",
    "    # Select action randomly or according to policy\n",
    "    if total_timesteps < start_timesteps:\n",
    "        action = np.random.uniform(-max_action, max_action, size=action_dim)\n",
    "    else:\n",
    "        action = (\n",
    "            policy.select_action(np.array(state))\n",
    "            + np.random.normal(0, max_action * expl_noise, size=action_dim)\n",
    "        ).clip(-max_action, max_action)\n",
    "\n",
    "    # Perform action\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action) \n",
    "    done = terminated or truncated or episode_timesteps >= max_steps\n",
    "    episode_reward += reward\n",
    "\n",
    "    # Store data in replay buffer\n",
    "    replay_buffer.add(state, action, next_state, reward, done)\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    if done or episode_timesteps >= max_steps: \n",
    "        print(f\"Episode Num: {episode_num + 1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
    "            # Train agent after collecting sufficient data\n",
    "        for _ in range(200):\n",
    "            policy.train(replay_buffer, batch_size)\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1 \n",
    "\n",
    "        # Evaluate episode\n",
    "        if episode_num % eval_freq == 0:\n",
    "            print(\"---------------------------------------\")\n",
    "            evaluations.append(eval_policy(policy, env, max_steps=max_steps))\n",
    "            print(\"---------------------------------------\")\n",
    "\n",
    "# Save the trained actor model\n",
    "torch.save(policy.actor.state_dict(), \"td3_pendulum_actor.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cccd040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Reward: -113.57\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "class TD3:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "# Main script\n",
    "env = gym.make(\"Pendulum-v1\", render_mode=\"human\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "policy = TD3(state_dim, action_dim, max_action)\n",
    "policy.actor.load_state_dict(torch.load(\"td3_pendulum_actor.pth\"))\n",
    "policy.actor.eval()\n",
    "\n",
    "state, _ = env.reset()\n",
    "episode_reward = 0\n",
    "for _ in range(500):\n",
    "    action = policy.select_action(np.array(state))\n",
    "    state, reward, terminated, truncated, _ = env.step(action)\n",
    "    episode_reward += reward\n",
    "    env.render()\n",
    "    time.sleep(0.01)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Episode Reward: {episode_reward:.2f}\")\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
